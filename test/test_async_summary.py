"""
æµ‹è¯•å¼‚æ­¥å†å²è®°å½•æ€»ç»“åŠŸèƒ½
"""
import sys
import os
import asyncio

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from internal.llm.llm_service import LLMService
from pkg.agent_prompt.agent_tool import knowledge_search


async def test_auto_summary():
    """æµ‹è¯•è‡ªåŠ¨æ€»ç»“åŠŸèƒ½"""
    print("=" * 80)
    print("æµ‹è¯•å¼‚æ­¥å†å²è®°å½•æ€»ç»“")
    print("=" * 80)
    
    # åˆ›å»ºæœåŠ¡ï¼Œè®¾ç½®è¾ƒå°çš„é˜ˆå€¼ä¾¿äºæµ‹è¯•
    llm = LLMService(
        model_name="deepseek-chat",
        model_type="cloud",
        auto_summary=True,
        max_history_count=8,  # 8æ¡æ¶ˆæ¯å°±è§¦å‘æ€»ç»“
        max_history_tokens=1000  # æˆ–è€…1000 token
    )
    
    print("\nâš™ï¸  é…ç½®:")
    print(f"  è‡ªåŠ¨æ€»ç»“: æ˜¯")
    print(f"  æœ€å¤§å†å²æ¡æ•°: 8")
    print(f"  æœ€å¤§å†å² token: 1000")
    
    # æ¨¡æ‹Ÿæ›´å¤šè½®å¯¹è¯ï¼Œå†…å®¹æ›´ä¸°å¯Œ
    conversations = [
        ("ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰ï¼Œæˆ‘æƒ³å­¦ä¹  FastAPI", 
         "ä½ å¥½å¼ ä¸‰ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚FastAPI æ˜¯ä¸€ä¸ªç°ä»£ã€å¿«é€Ÿçš„ Web æ¡†æ¶ï¼Œç”¨äºæ„å»º APIã€‚å®ƒåŸºäº Python 3.6+ çš„ç±»å‹æç¤ºï¼Œæ€§èƒ½æ¥è¿‘ NodeJS å’Œ Goã€‚ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼šè‡ªåŠ¨æ–‡æ¡£ç”Ÿæˆã€æ•°æ®éªŒè¯ã€å¼‚æ­¥æ”¯æŒç­‰ã€‚"),
        
        ("FastAPI å’Œ Flask æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ", 
         "FastAPI å’Œ Flask çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼š1) FastAPI åŸç”Ÿæ”¯æŒå¼‚æ­¥ï¼Œæ€§èƒ½æ›´å¥½ï¼›2) FastAPI åŸºäºç±»å‹æç¤ºï¼Œè‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£å’Œæ•°æ®éªŒè¯ï¼›3) Flask æ›´è½»é‡ï¼Œç¤¾åŒºæ›´æˆç†Ÿï¼›4) FastAPI æ›´é€‚åˆæ„å»ºç°ä»£ RESTful API å’Œå¾®æœåŠ¡ã€‚"),
        
        ("æˆ‘æƒ³ç”¨ FastAPI å¼€å‘ä¸€ä¸ªç”¨æˆ·ç®¡ç†ç³»ç»Ÿï¼Œåº”è¯¥å¦‚ä½•è®¾è®¡æ•°æ®åº“ï¼Ÿ", 
         "å¼€å‘ç”¨æˆ·ç®¡ç†ç³»ç»Ÿï¼Œå»ºè®®ä½¿ç”¨ä»¥ä¸‹æ•°æ®åº“è®¾è®¡ï¼š1) ç”¨æˆ·è¡¨ (users)ï¼šåŒ…å« idã€usernameã€emailã€password_hashã€created_at ç­‰å­—æ®µï¼›2) è§’è‰²è¡¨ (roles)ï¼šidã€nameã€permissionsï¼›3) ç”¨æˆ·è§’è‰²å…³è”è¡¨ï¼›4) ä½¿ç”¨ SQLAlchemy æˆ– Tortoise ORM ä½œä¸º ORM æ¡†æ¶ï¼›5) å¯†ç ä½¿ç”¨ bcrypt åŠ å¯†å­˜å‚¨ã€‚"),
        
        ("èƒ½è¯¦ç»†è¯´è¯´ FastAPI çš„ä¾èµ–æ³¨å…¥å—ï¼Ÿ", 
         "FastAPI çš„ä¾èµ–æ³¨å…¥éå¸¸å¼ºå¤§ï¼å®ƒé€šè¿‡ Depends() å®ç°ã€‚ä¸»è¦ç”¨é€”ï¼š1) å…±äº«æ•°æ®åº“è¿æ¥ï¼›2) ç”¨æˆ·è®¤è¯ï¼›3) å‚æ•°éªŒè¯ï¼›4) æƒé™æ£€æŸ¥ã€‚ä¾‹å¦‚ï¼šdef get_db(): return database; @app.get('/users'); async def read_users(db = Depends(get_db)): ...ã€‚ä¾èµ–é¡¹å¯ä»¥åµŒå¥—ï¼Œå½¢æˆä¾èµ–æ ‘ã€‚"),
        
        ("æˆ‘çš„é¡¹ç›®éœ€è¦å®ç° JWT è®¤è¯ï¼Œæœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ", 
         "å®ç° JWT è®¤è¯çš„å»ºè®®ï¼š1) ä½¿ç”¨ python-jose åº“ç”Ÿæˆå’ŒéªŒè¯ tokenï¼›2) åˆ›å»º /login ç«¯ç‚¹è¿”å› access_tokenï¼›3) ä½¿ç”¨ Depends(get_current_user) ä¿æŠ¤éœ€è¦è®¤è¯çš„è·¯ç”±ï¼›4) è®¾ç½®åˆç†çš„è¿‡æœŸæ—¶é—´ï¼ˆå¦‚30åˆ†é’Ÿï¼‰ï¼›5) è€ƒè™‘å®ç° refresh token æœºåˆ¶ï¼›6) æ•æ„Ÿæ“ä½œéœ€è¦äºŒæ¬¡éªŒè¯ã€‚"),
        
        ("åœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•è°ƒè¯• FastAPI åº”ç”¨ï¼Ÿ", 
         "è°ƒè¯• FastAPI åº”ç”¨çš„æ–¹æ³•ï¼š1) ä½¿ç”¨ uvicorn --reload å®ç°çƒ­é‡è½½ï¼›2) åˆ©ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ /docs æ¥å£æµ‹è¯• APIï¼›3) ä½¿ç”¨ logging æˆ– loguru è®°å½•æ—¥å¿—ï¼›4) PyCharm/VSCode é…ç½®æ–­ç‚¹è°ƒè¯•ï¼›5) ä½¿ç”¨ pytest ç¼–å†™å•å…ƒæµ‹è¯•ï¼›6) åˆ©ç”¨ FastAPI çš„å¼‚å¸¸å¤„ç†æœºåˆ¶æ•è·é”™è¯¯ã€‚"),
        
        ("é¡¹ç›®éƒ¨ç½²æ—¶ï¼Œç”Ÿäº§ç¯å¢ƒåº”è¯¥æ³¨æ„ä»€ä¹ˆï¼Ÿ", 
         "ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ³¨æ„äº‹é¡¹ï¼š1) ä½¿ç”¨ gunicorn + uvicorn worker è¿è¡Œï¼›2) é…ç½® Nginx åå‘ä»£ç†ï¼›3) å¯ç”¨ HTTPSï¼ˆLet's Encryptï¼‰ï¼›4) è®¾ç½®ç¯å¢ƒå˜é‡ç®¡ç†é…ç½®ï¼›5) ä½¿ç”¨ Docker å®¹å™¨åŒ–éƒ¨ç½²ï¼›6) é…ç½®æ—¥å¿—æ”¶é›†ï¼›7) è®¾ç½®ç›‘æ§å‘Šè­¦ï¼ˆå¦‚ Prometheusï¼‰ï¼›8) å®šæœŸå¤‡ä»½æ•°æ®åº“ã€‚"),
        
        ("éå¸¸æ„Ÿè°¢ä½ çš„è¯¦ç»†è§£ç­”ï¼Œæˆ‘å­¦åˆ°äº†å¾ˆå¤šï¼", 
         "ä¸å®¢æ°”ï¼Œå¼ ä¸‰ï¼å¾ˆé«˜å…´èƒ½å¸®åˆ°ä½ ã€‚FastAPI æ˜¯ä¸€ä¸ªéå¸¸ä¼˜ç§€çš„æ¡†æ¶ï¼Œå»ºè®®ä½ ï¼š1) å¤šçœ‹å®˜æ–¹æ–‡æ¡£ï¼›2) å®è·µå°é¡¹ç›®ï¼›3) å…³æ³¨å¼‚æ­¥ç¼–ç¨‹ï¼›4) å­¦ä¹ ç±»å‹æç¤ºã€‚ç¥ä½ å¼€å‘é¡ºåˆ©ï¼æœ‰é—®é¢˜éšæ—¶é—®æˆ‘ã€‚")
    ]
    
    print("\n" + "=" * 80)
    print("ğŸ“ ç¬¬ä¸€é˜¶æ®µï¼šæ¨¡æ‹Ÿå¤šè½®å¯¹è¯ï¼ˆä¼šè§¦å‘è‡ªåŠ¨æ€»ç»“ï¼‰")
    print("=" * 80)
    
    for i, (user_msg, assistant_msg) in enumerate(conversations, 1):
        print(f"\nç¬¬ {i} è½®å¯¹è¯:")
        print(f"  ğŸ‘¤ ç”¨æˆ·: {user_msg}")
        print(f"  ğŸ¤– åŠ©æ‰‹: {assistant_msg[:60]}...")
        
        # æ·»åŠ åˆ°å†å²
        llm.add_to_history("user", user_msg)
        llm.add_to_history("assistant", assistant_msg)
        
        # æ˜¾ç¤ºç»Ÿè®¡
        stats = llm.get_history_stats()
        print(f"  ğŸ“Š å†å²è®°å½•: {stats['count']} æ¡, {stats['total_tokens']} tokens")
        
        # ç­‰å¾…ä¸€ä¸‹ï¼Œè®©å¼‚æ­¥æ€»ç»“æœ‰æœºä¼šæ‰§è¡Œ
        await asyncio.sleep(0.5)
        
        # å¦‚æœæ­£åœ¨æ€»ç»“
        if stats['is_summarizing']:
            print(f"  â³ è§¦å‘æ€»ç»“ï¼æ­£åœ¨åå°æ€»ç»“å†å²è®°å½•...")
            await asyncio.sleep(3)  # ç­‰å¾…æ€»ç»“å®Œæˆ
    
    # ç¡®ä¿æ€»ç»“å®Œæˆ
    print("\nâ³ ç­‰å¾…æ€»ç»“å®Œæˆ...")
    await asyncio.sleep(2)
    
    # æ˜¾ç¤ºæ€»ç»“åçš„çŠ¶æ€
    print("\n" + "=" * 80)
    print("ğŸ“‹ æ€»ç»“åçš„å†å²è®°å½•çŠ¶æ€")
    print("=" * 80)
    
    final_stats = llm.get_history_stats()
    print(f"\nå†å²è®°å½•ç»Ÿè®¡:")
    print(f"  æ¡æ•°: {final_stats['count']}/{final_stats['max_count']}")
    print(f"  Token: {final_stats['total_tokens']}/{final_stats['max_tokens']}")
    
    history = llm.get_history()
    print(f"\nå½“å‰å†å²è®°å½•å†…å®¹ (å…± {len(history)} æ¡):")
    print("-" * 80)
    for i, msg in enumerate(history, 1):
        role_icon = "ğŸ‘¤" if msg['role'] == 'user' else "ğŸ¤–" if msg['role'] == 'assistant' else "ğŸ“"
        print(f"\n{role_icon} [{msg['role'].upper()}]:")
        print(f"{msg['content']}")
        print("-" * 80)
    
    # ç¬¬äºŒé˜¶æ®µï¼šåŸºäºæ€»ç»“çš„å†å²ç»§ç»­å¯¹è¯
    print("\n" + "=" * 80)
    print("ğŸ”„ ç¬¬äºŒé˜¶æ®µï¼šéªŒè¯ AI æ˜¯å¦è®°å¾—æ€»ç»“çš„å†…å®¹")
    print("=" * 80)
    
    # æµ‹è¯•é—®é¢˜ï¼šéªŒè¯ AI æ˜¯å¦è®°å¾—ä¹‹å‰å¯¹è¯çš„å†…å®¹
    test_questions = [
        "æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ",
        "æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡æ•°æ®åº“è®¾è®¡å—ï¼Ÿ",
        "ä½ ç»™æˆ‘æ¨èäº†å“ªäº›éƒ¨ç½²æ–¹æ¡ˆï¼Ÿ"
    ]
    
    for question in test_questions:
        print(f"\nğŸ‘¤ ç”¨æˆ·è¿½é—®: {question}")
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«å†å²è®°å½•ï¼‰
        messages = llm.get_history().copy()
        messages.append({"role": "user", "content": question})
        
        print(f"ğŸ¤– åŠ©æ‰‹å›ç­”: ", end="", flush=True)
        
        # æµå¼è¾“å‡ºå›ç­”
        response_text = ""
        try:
            for chunk in llm.chat(messages, stream=True):
                print(chunk, end="", flush=True)
                response_text += chunk
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            response_text = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
        
        print()  # æ¢è¡Œ
        
        # æ·»åŠ åˆ°å†å²
        llm.add_to_history("user", question)
        llm.add_to_history("assistant", response_text)
        
        await asyncio.sleep(1)
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ è§‚å¯Ÿè¦ç‚¹ï¼š")
    print("  1. æ€»ç»“æ˜¯å¦ä¿ç•™äº†å…³é”®ä¿¡æ¯ï¼ˆç”¨æˆ·åã€è®¨è®ºä¸»é¢˜ç­‰ï¼‰")
    print("  2. AI åœ¨åç»­å¯¹è¯ä¸­æ˜¯å¦èƒ½åŸºäºæ€»ç»“å›ç­”é—®é¢˜")
    print("  3. å†å²è®°å½•æ•°é‡æ˜¯å¦å¤§å¹…å‡å°‘ï¼ˆä»å¤šæ¡å‹ç¼©åˆ°å°‘é‡æ€»ç»“ï¼‰")


async def test_manual_summary():
    """æµ‹è¯•æ‰‹åŠ¨è§¦å‘æ€»ç»“"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ‰‹åŠ¨æ€»ç»“")
    print("=" * 60)
    
    llm = LLMService(
        model_name="deepseek-chat",
        model_type="cloud",
        auto_summary=False  # å…³é—­è‡ªåŠ¨æ€»ç»“
    )
    
    # æ·»åŠ ä¸€äº›å†å²
    llm.add_to_history("user", "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
    llm.add_to_history("assistant", "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯...")
    llm.add_to_history("user", "å®ƒæœ‰å“ªäº›åº”ç”¨ï¼Ÿ")
    llm.add_to_history("assistant", "åº”ç”¨åŒ…æ‹¬ï¼šå›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«...")
    
    print(f"\næ·»åŠ å†å²å‰: {len(llm.get_history())} æ¡")
    
    # æ‰‹åŠ¨è§¦å‘æ€»ç»“
    print("\næ‰‹åŠ¨è§¦å‘æ€»ç»“...")
    await llm._summarize_history_async()
    
    print(f"æ€»ç»“å: {len(llm.get_history())} æ¡")
    
    history = llm.get_history()
    if history:
        print(f"\næ€»ç»“å†…å®¹:\n{history[0]['content']}")



async def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸš€ å¼€å§‹æµ‹è¯•å¼‚æ­¥å†å²è®°å½•æ€»ç»“åŠŸèƒ½\n")
    
    # æµ‹è¯•è‡ªåŠ¨æ€»ç»“
    await test_auto_summary()
    
    # æµ‹è¯•æ‰‹åŠ¨æ€»ç»“
    # await test_manual_summary()
    
    # æµ‹è¯• token é™åˆ¶
    # await test_token_limit()
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼\n")


if __name__ == "__main__":
    asyncio.run(main())

