"""
æµ‹è¯•ç®€åŒ–ç‰ˆ LLM æœåŠ¡
æ¼”ç¤ºæç¤ºè¯å’Œå·¥å…·çš„é…å¯¹ä½¿ç”¨
"""
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from internal.llm.llm_service import LLMService
from pkg.agent_prompt.agent_tool import (
    list_all_tools,
    knowledge_search,
    document_analyzer,
    code_executor
)
from pkg.agent_prompt.prompt_templates import (
    DEFAULT_PROMPT,
    RAG_PROMPT,
    CODE_PROMPT,
    DOCUMENT_PROMPT
)


def test_basic_chat():
    """æµ‹è¯•åŸºç¡€å¯¹è¯"""
    print("=" * 60)
    print("1ï¸âƒ£ æµ‹è¯•åŸºç¡€å¯¹è¯ï¼ˆæ— å·¥å…·ï¼‰")
    print("=" * 60)
    
    try:
        # ä¸ä½¿ç”¨ä»»ä½•å·¥å…·
        llm = LLMService(
            model_name="llama3.2",
            model_type="local"
        )
        
        # æ˜¾ç¤ºé…ç½®
        info = llm.get_info()
        print(f"\næ¨¡å‹: {info['model_name']} ({info['model_type']})")
        print(f"å·¥å…·: {info['tools'] if info['tools'] else 'æ— '}")
        print(f"æç¤ºè¯: {info['system_prompt'][:80]}...")
        
        # å¯¹è¯
        messages = [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±"}]
        
        print(f"\nç”¨æˆ·: {messages[0]['content']}")
        print("åŠ©æ‰‹: ", end="", flush=True)
        
        for chunk in llm.chat(messages, stream=True):
            print(chunk, end="", flush=True)
        print("\n")
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")


def test_rag_with_tool():
    """æµ‹è¯• RAG å·¥å…·"""
    print("\n" + "=" * 60)
    print("2ï¸âƒ£ æµ‹è¯• RAG å·¥å…·ï¼ˆçŸ¥è¯†åº“æ£€ç´¢ï¼‰")
    print("=" * 60)
    
    try:
        # ä½¿ç”¨çŸ¥è¯†åº“æœç´¢å·¥å…·ï¼ˆå¯ä»¥ç›´æ¥ç‚¹å‡»å‡½æ•°åè·³è½¬ï¼‰
        llm = LLMService(
            model_name="llama3.2",
            model_type="local",
            tools=[knowledge_search]  # ç›´æ¥ä¼ å…¥å‡½æ•°å¼•ç”¨ï¼ŒIDE å¯ä»¥ç‚¹å‡»è·³è½¬
        )
        
        info = llm.get_info()
        print(f"\næ¨¡å‹: {info['model_name']}")
        print(f"å·¥å…·: {info['tools']}")
        print(f"æç¤ºè¯: {info['system_prompt'][:80]}...")
        
        # æ¨¡æ‹ŸçŸ¥è¯†åº“ä¸Šä¸‹æ–‡
        knowledge_context = """
ã€çŸ¥è¯†ç‰‡æ®µ1ã€‘
RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”ŸæˆæŠ€æœ¯ã€‚

ã€çŸ¥è¯†ç‰‡æ®µ2ã€‘
RAG çš„ä¼˜åŠ¿ï¼šå‡å°‘å¹»è§‰ã€æä¾›å¯è¿½æº¯æ¥æºã€æ— éœ€é‡è®­ç»ƒã€‚
"""
        
        messages = [{"role": "user", "content": "ä»€ä¹ˆæ˜¯RAGï¼Ÿ"}]
        
        print(f"\nç”¨æˆ·: {messages[0]['content']}")
        print("åŠ©æ‰‹: ", end="", flush=True)
        
        for chunk in llm.chat(messages, context=knowledge_context, stream=True):
            print(chunk, end="", flush=True)
        print("\n")
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")


def test_code_tool():
    """æµ‹è¯•ä»£ç å·¥å…·"""
    print("\n" + "=" * 60)
    print("3ï¸âƒ£ æµ‹è¯•ä»£ç å·¥å…·")
    print("=" * 60)
    
    try:
        # ä½¿ç”¨ä»£ç æ‰§è¡Œå·¥å…·ï¼ˆå¯ä»¥ Cmd+ç‚¹å‡»è·³è½¬ï¼‰
        llm = LLMService(
            model_name="llama3.2",
            model_type="local",
            tools=[code_executor]  # ç›´æ¥ä¼ å…¥å‡½æ•°å¼•ç”¨
        )
        
        info = llm.get_info()
        print(f"\næ¨¡å‹: {info['model_name']}")
        print(f"å·¥å…·: {info['tools']}")
        print(f"æç¤ºè¯æ¨¡æ¿: code")
        
        messages = [{"role": "user", "content": "å¦‚ä½•ä¼˜åŒ–Pythonä»£ç æ€§èƒ½ï¼Ÿ"}]
        
        print(f"\nç”¨æˆ·: {messages[0]['content']}")
        print("åŠ©æ‰‹: ", end="", flush=True)
        
        for chunk in llm.chat(messages, stream=True):
            print(chunk, end="", flush=True)
        print("\n")
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")


def test_custom_prompt():
    """æµ‹è¯•è‡ªå®šä¹‰æç¤ºè¯"""
    print("\n" + "=" * 60)
    print("4ï¸âƒ£ æµ‹è¯•è‡ªå®šä¹‰æç¤ºè¯")
    print("=" * 60)
    
    try:
        # ç›´æ¥ä½¿ç”¨æç¤ºè¯å¸¸é‡ï¼ˆå¯ä»¥ Cmd+ç‚¹å‡»è·³è½¬ï¼‰
        llm = LLMService(
            model_name="llama3.2",
            model_type="local",
            system_prompt=DEFAULT_PROMPT,  # ç‚¹å‡»å¯è·³è½¬åˆ°æç¤ºè¯å®šä¹‰
            tools=[knowledge_search, document_analyzer]  # å¯ä»¥ä¼ å…¥å¤šä¸ªå·¥å…·
        )
        
        info = llm.get_info()
        print(f"\næ¨¡å‹: {info['model_name']}")
        print(f"å·¥å…·: {info['tools']}")
        print(f"è‡ªå®šä¹‰æç¤ºè¯: æ˜¯")
        
        messages = [{"role": "user", "content": "æ— äººæœºå¦‚ä½•å®ç°è‡ªä¸»å¯¼èˆªï¼Ÿ"}]
        
        print(f"\nç”¨æˆ·: {messages[0]['content']}")
        print("åŠ©æ‰‹: ", end="", flush=True)
        
        for chunk in llm.chat(messages, stream=True):
            print(chunk, end="", flush=True)
        print("\n")
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")


def test_list_tools():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨å·¥å…·"""
    print("\n" + "=" * 60)
    print("5ï¸âƒ£ æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å·¥å…·")
    print("=" * 60)
    
    tools = list_all_tools()
    print(f"\nå…±æœ‰ {len(tools)} ä¸ªå¯ç”¨å·¥å…·:\n")
    
    for tool in tools:
        print(f"å·¥å…·å: {tool['name']}")
        print(f"  æè¿°: {tool['description']}")
        print(f"  æç¤ºè¯: {tool['prompt_template']}")
        print()


def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸš€ å¼€å§‹æµ‹è¯•ç®€åŒ–ç‰ˆ LLM æœåŠ¡\n")
    
    # æµ‹è¯•åŸºç¡€å¯¹è¯
    test_basic_chat()
    
    # æµ‹è¯• RAG å·¥å…·
    test_rag_with_tool()
    
    # æµ‹è¯•ä»£ç å·¥å…·
    test_code_tool()
    
    # æµ‹è¯•è‡ªå®šä¹‰æç¤ºè¯
    test_custom_prompt()
    
    # åˆ—å‡ºæ‰€æœ‰å·¥å…·
    test_list_tools()
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼\n")


if __name__ == "__main__":
    main()

