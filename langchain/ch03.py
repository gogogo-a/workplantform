# reactèŒƒå¼
OPENAI_API_KEY="sk-6fc6f53cc4584663b7926f469f4b4a4d"
base_url="https://api.deepseek.com"

from langchain_openai import ChatOpenAI
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain.agents import create_agent
from fastapi import APIRouter, Request, FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from contextlib import asynccontextmanager

# åˆå§‹åŒ– LLM
llm = ChatOpenAI(
    model="deepseek-chat",
    openai_api_key=OPENAI_API_KEY,
    openai_api_base=base_url,
    temperature=0.3,
    max_tokens=2048,
    stream_usage=True
)
# from langchain_community.llms import Ollama
# llm = Ollama(base_url="http://localhost:11434",model="llama3.2")
# MCP å·¥å…·é…ç½®
tools_path=[
    '/Users/haogeng/Desktop/genghao/work2/worktest/plantform/langchain/tools/web_search.py',
    '/Users/haogeng/Desktop/genghao/work2/worktest/plantform/langchain/tools/weather_query.py'
]

# ä¸ºæ¯ä¸ªå·¥å…·åˆ›å»ºæœåŠ¡å™¨å‚æ•°
server_params_list = [
    StdioServerParameters(
        command='/Users/haogeng/miniforge3/envs/langchain/bin/python',
        args=[tool_path],
    ) for tool_path in tools_path
]

# å…¨å±€å˜é‡
mcp_clients = []
mcp_sessions = []
tools = []

@asynccontextmanager
async def lifespan(app: FastAPI):
    # å¯åŠ¨æ—¶åˆå§‹åŒ– MCP
    global mcp_clients, mcp_sessions, tools
    print("ğŸš€ å¯åŠ¨å¤šä¸ª MCP è¿æ¥...")
    
    # ä¸ºæ¯ä¸ªå·¥å…·å¯åŠ¨ç‹¬ç«‹çš„ MCP æœåŠ¡å™¨
    for i, server_params in enumerate(server_params_list):
        print(f"ğŸ”§ å¯åŠ¨å·¥å…·æœåŠ¡å™¨ {i+1}/{len(server_params_list)}: {tools_path[i]}")
        
        mcp_client = stdio_client(server_params)
        read, write = await mcp_client.__aenter__()
        mcp_session = ClientSession(read, write)
        await mcp_session.__aenter__()
        await mcp_session.initialize()
        
        # åŠ è½½å½“å‰æœåŠ¡å™¨çš„å·¥å…·
        server_tools = await load_mcp_tools(mcp_session)
        tools.extend(server_tools)
        
        # ä¿å­˜å®¢æˆ·ç«¯å’Œä¼šè¯
        mcp_clients.append(mcp_client)
        mcp_sessions.append(mcp_session)
    
    print(f"âœ… æ‰€æœ‰ MCP å·¥å…·åŠ è½½æˆåŠŸ: {[t.name for t in tools]}")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†æ‰€æœ‰ MCP è¿æ¥
    print("ğŸ”„ å…³é—­æ‰€æœ‰ MCP è¿æ¥...")
    for i, (session, client) in enumerate(zip(mcp_sessions, mcp_clients)):
        print(f"ğŸ”„ å…³é—­è¿æ¥ {i+1}/{len(mcp_sessions)}")
        if session:
            await session.__aexit__(None, None, None)
        if client:
            await client.__aexit__(None, None, None)

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(lifespan=lifespan)
router = APIRouter()

template = """Answer the following questions as best you can. You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: {input}
Thought:{agent_scratchpad}"""
    


class QueryRequest(BaseModel):
    query: str

@router.post("/query")
async def query(request: QueryRequest):
    """ä½¿ç”¨ ReAct Agent å¤„ç†æŸ¥è¯¢ï¼ˆæµå¼è¿”å›ï¼‰"""
    async def generate():
        try:
            # åœ¨è¯·æ±‚æ—¶åˆ›å»º Agentï¼ˆç¡®ä¿å·¥å…·å·²åŠ è½½ï¼‰
            if not tools:
                yield "å·¥å…·æœªåŠ è½½ï¼Œè¯·ç¨åé‡è¯•"
                return
                
            agent = create_agent(llm, tools, system_prompt=template)
            
            # æµå¼æ‰§è¡Œ Agent - å±•ç¤º ReAct æ€è€ƒè¿‡ç¨‹
            async for chunk in agent.astream({"messages": [("user", request.query)]}):
                print(f"Chunk: {chunk}")  # è°ƒè¯•ä¿¡æ¯
                
                # å¤„ç† Agent çš„æ€è€ƒå’Œè¡ŒåŠ¨æ­¥éª¤
                for node_name, node_data in chunk.items():
                    if "messages" in node_data:
                        for message in node_data["messages"]:
                            if hasattr(message, 'content') and message.content:
                                # æ ¹æ®èŠ‚ç‚¹ç±»å‹æ·»åŠ æ ‡è¯†
                                if node_name == "agent":
                                    yield f"\nğŸ¤” **æ€è€ƒ**: "
                                elif node_name == "tools":
                                    yield f"\nğŸ”§ **å·¥å…·æ‰§è¡Œç»“æœ**: "
                                else:
                                    yield f"\nğŸ“ **{node_name}**: "
                                
                                # é€å­—ç¬¦æµå¼è¾“å‡ºå†…å®¹
                                for char in message.content:
                                    print(message)
                                    yield char
                                    
                                yield "\n"  # æ¯ä¸ªæ­¥éª¤åæ¢è¡Œ
                
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"è¯¦ç»†é”™è¯¯: {error_detail}")
            yield f"å¤„ç†å¤±è´¥: {str(e)}\nè¯¦ç»†é”™è¯¯: {error_detail}"
    
    return StreamingResponse(generate(), media_type="text/plain")


# æ³¨å†Œè·¯ç”±
app.include_router(router)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8081)
