"""
æ–‡æ¡£å¤„ç†æœåŠ¡
ç»Ÿä¸€çš„æ–‡æ¡£å¤„ç†å™¨ï¼Œæ•´åˆæ–‡æ¡£åŠ è½½ã€åˆ†å‰²ã€Embeddingã€å­˜å‚¨ç­‰åŠŸèƒ½
æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥å¤„ç†ï¼ˆChannel/Kafkaï¼‰
"""
from typing import Dict, Any, List, Optional
from pathlib import Path
from log import logger
from internal.document_client.message_client import message_client
from internal.document_client.config_loader import config
from internal.embedding.embedding_service import embedding_service
from internal.db.milvus import milvus_client
from pkg.constants.constants import MILVUS_COLLECTION_NAME
from internal.document_client.document_extract import extractor_manager
from internal.monitor import record_performance  # ğŸ”¥ å¯¼å…¥æ€§èƒ½ç›‘æ§


class DocumentProcessor:
    """
    ç»Ÿä¸€æ–‡æ¡£å¤„ç†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    åŠŸèƒ½ï¼š
    1. æ–‡æ¡£åŠ è½½ï¼ˆPDF, DOCX, TXTï¼‰
    2. æ–‡æœ¬åˆ†å‰²
    3. Embedding ç”Ÿæˆ
    4. å­˜å‚¨åˆ° Milvus
    5. å¼‚æ­¥ä»»åŠ¡å¤„ç†ï¼ˆChannel/Kafkaï¼‰
    """
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
        
        self.embedding_config = config.embedding_config
        self.milvus_config = config.milvus_config
        self.message_client = message_client
        
        # ä»é…ç½®è¯»å–åˆ†å—å‚æ•°
        self.chunk_size = self.embedding_config.get('chunk_size', 500)
        self.chunk_overlap = self.embedding_config.get('chunk_overlap', 50)
        
        self._initialized = True
        logger.info(
            f"æ–‡æ¡£å¤„ç†å™¨å·²åˆå§‹åŒ– "
            f"(åˆ†å—å¤§å°: {self.chunk_size}, é‡å : {self.chunk_overlap})"
        )
    
    # ==================== åŒæ­¥å¤„ç†æ–¹æ³• ====================
    
    def process_file(
        self,
        file_path: str,
        document_uuid: str,
        collection_name: Optional[str] = None,
        extra_metadata: Optional[Dict[str, Any]] = None,
        permission: int = 0
    ) -> Dict[str, Any]:
        """
        åŒæ­¥å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆåŠ è½½ -> åˆ†å‰² -> Embedding -> å­˜å‚¨ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            document_uuid: æ–‡æ¡£ UUID
            collection_name: Milvus é›†åˆåç§°ï¼ˆå¯é€‰ï¼‰
            extra_metadata: é¢å¤–å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            permission: æ–‡æ¡£æƒé™ï¼ˆ0=æ™®é€šç”¨æˆ·å¯è§ï¼Œ1=ä»…ç®¡ç†å‘˜å¯è§ï¼‰
        
        Returns:
            Dict: å¤„ç†ç»“æœ {success, message, chunks_count, vectors_count, embedding_time, processing_time}
        """
        try:
            import time
            from datetime import datetime
            
            # è®°å½•å¤„ç†å¼€å§‹æ—¶é—´
            process_start_time = time.time()
            start_datetime = datetime.now()
            # 1. éªŒè¯æ–‡ä»¶
            if not extractor_manager.is_supported(file_path):
                return {
                    "success": False,
                    "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {Path(file_path).suffix}"
                }
            
            file_info = extractor_manager.get_file_info(file_path)
            logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_info['name']}, UUID: {document_uuid}")
            
            # 2. åŠ è½½æ–‡æ¡£
            loaded_docs = extractor_manager.load_document(file_path)
            full_content = "\n\n".join([doc["content"] for doc in loaded_docs])
            
            # 3. åˆ†å‰²æ–‡æœ¬
            chunks = extractor_manager.split_text(
                text=full_content,
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                metadata={
                    "document_uuid": document_uuid,
                    "filename": file_info['name'],
                    "source": file_path,
                    "permission": permission,  # ğŸ”¥ æ·»åŠ æƒé™åˆ°å…ƒæ•°æ®
                    **(extra_metadata or {})
                }
            )
            
            if not chunks:
                return {
                    "success": False,
                    "message": "æ–‡æ¡£åˆ†å‰²åæ²¡æœ‰ç”Ÿæˆæ–‡æœ¬å—"
                }
            
            logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆ: {len(chunks)} ä¸ªå—")
            
            # 4. æ‰¹é‡ Embeddingï¼ˆè®°å½•æ—¶é—´ï¼‰
            embedding_start_time = time.time()
            texts = [chunk["content"] for chunk in chunks]
            embeddings = embedding_service.encode_documents(
                documents=texts,
                batch_size=self.embedding_config.get('batch_size', 32),
                normalize=True
            )
            embedding_duration = time.time() - embedding_start_time
            
            logger.info(f"Embedding ç”Ÿæˆå®Œæˆ: {len(embeddings)} ä¸ªå‘é‡, è€—æ—¶: {embedding_duration:.2f}ç§’")
            
            # ğŸ”¥ è®°å½• Embedding æ€§èƒ½ç›‘æ§
            total_text = "\n".join(texts)
            text_length = len(total_text)
            # ä¼°ç®— token æ•°é‡ï¼ˆä¸­è‹±æ–‡æ··åˆï¼šå­—ç¬¦æ•° * 0.8ï¼‰
            token_count = int(text_length * 0.8)
            
            record_performance(
                monitor_type="embedding",
                operation=f"æ–‡æ¡£å‘é‡åŒ–_{file_info['name']}",
                duration=embedding_duration,
                token_count=token_count,  # ğŸ”¥ ä¼ å…¥ token_countï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è®¡ç®— tokens/s å’Œ ms/10k tokens
                document_uuid=document_uuid,
                filename=file_info['name'],
                chunks_count=len(chunks),
                vectors_count=len(embeddings),
                text_length=text_length
            )
            
            # 5. å‡†å¤‡ Milvus æ•°æ®
            texts = []
            metadata_list = []
            
            for i, chunk in enumerate(chunks):
                texts.append(chunk["content"])
                metadata_list.append({
                    "document_uuid": document_uuid,
                    "chunk_index": i,
                    "chunk_count": len(chunks),
                    "filename": file_info['name'],
                    "source": file_path,
                    "permission": permission,  # ğŸ”¥ ç¡®ä¿æ¯ä¸ªchunkéƒ½æœ‰permission
                    **chunk["metadata"]
                })
            
            # 6. å­˜å‚¨åˆ° Milvus
            if collection_name is None:
                collection_name = MILVUS_COLLECTION_NAME
            
            # ç¡®ä¿ collection å­˜åœ¨
            existing_collections = milvus_client.list_collections()
            if collection_name not in existing_collections:
                dimension = self.milvus_config.get('dimension', 1024)
                logger.info(f"åˆ›å»º Milvus collection: {collection_name}, ç»´åº¦: {dimension}")
                milvus_client.create_collection(
                    collection_name=collection_name,
                    dimension=dimension,
                    description="æ–‡æ¡£å‘é‡å­˜å‚¨",
                    metric_type="COSINE"
                )
            
            # è½¬æ¢ embeddings ä¸ºåˆ—è¡¨
            embeddings_list = [emb.tolist() for emb in embeddings]
            
            ids = milvus_client.insert_vectors(
                collection_name=collection_name,
                embeddings=embeddings_list,
                texts=texts,
                metadata=metadata_list
            )
            success = ids is not None and len(ids) > 0
            
            # è®¡ç®—æ€»å¤„ç†æ—¶é—´
            process_duration = time.time() - process_start_time
            complete_datetime = datetime.now()
            
            if success:
                logger.info(
                    f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {file_info['name']}, "
                    f"UUID: {document_uuid}, "
                    f"å—æ•°: {len(chunks)}, "
                    f"å‘é‡æ•°: {len(embeddings)}, "
                    f"Embeddingè€—æ—¶: {embedding_duration:.2f}ç§’, "
                    f"æ€»è€—æ—¶: {process_duration:.2f}ç§’"
                )
                return {
                    "success": True,
                    "message": "å¤„ç†æˆåŠŸ",
                    "chunks_count": len(chunks),
                    "vectors_count": len(embeddings),
                    "document_uuid": document_uuid,
                    "embedding_time": round(embedding_duration, 2),  # ğŸ”¥ embeddingæ—¶é—´ï¼ˆç§’ï¼‰
                    "processing_time": round(process_duration, 2),  # ğŸ”¥ æ€»å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
                    "start_datetime": start_datetime.isoformat(),  # ğŸ”¥ å¼€å§‹æ—¶é—´
                    "complete_datetime": complete_datetime.isoformat()  # ğŸ”¥ å®Œæˆæ—¶é—´
                }
            else:
                return {
                    "success": False,
                    "message": "å­˜å‚¨åˆ° Milvus å¤±è´¥"
                }
                
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "message": f"å¤„ç†å¼‚å¸¸: {str(e)}"
            }
    
    def process_text(
        self,
        text: str,
        document_uuid: str,
        collection_name: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        åŒæ­¥å¤„ç†çº¯æ–‡æœ¬ï¼ˆåˆ†å‰² -> Embedding -> å­˜å‚¨ï¼‰
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            document_uuid: æ–‡æ¡£ UUID
            collection_name: Milvus é›†åˆåç§°ï¼ˆå¯é€‰ï¼‰
            metadata: å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            Dict: å¤„ç†ç»“æœ
        """
        try:
            import time  # ğŸ”¥ å¯¼å…¥timeæ¨¡å—
            
            logger.info(f"å¼€å§‹å¤„ç†æ–‡æœ¬ï¼ŒUUID: {document_uuid}")
            
            # 1. åˆ†å‰²æ–‡æœ¬
            chunks = extractor_manager.split_text(
                text=text,
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                metadata={
                    "document_uuid": document_uuid,
                    **(metadata or {})
                }
            )
            
            if not chunks:
                return {
                    "success": False,
                    "message": "æ–‡æœ¬åˆ†å‰²åæ²¡æœ‰ç”Ÿæˆæ–‡æœ¬å—"
                }
            
            # 2. æ‰¹é‡ Embeddingï¼ˆè®°å½•æ—¶é—´ï¼‰
            embedding_start_time = time.time()
            texts = [chunk["content"] for chunk in chunks]
            embeddings = embedding_service.encode_documents(
                documents=texts,
                batch_size=self.embedding_config.get('batch_size', 32),
                normalize=True
            )
            embedding_duration = time.time() - embedding_start_time
            
            logger.info(f"Embedding ç”Ÿæˆå®Œæˆ: {len(embeddings)} ä¸ªå‘é‡, è€—æ—¶: {embedding_duration:.2f}ç§’")
            
            # ğŸ”¥ è®°å½• Embedding æ€§èƒ½ç›‘æ§
            total_text = "\n".join(texts)
            text_length = len(total_text)
            # ä¼°ç®— token æ•°é‡ï¼ˆä¸­è‹±æ–‡æ··åˆï¼šå­—ç¬¦æ•° * 0.8ï¼‰
            token_count = int(text_length * 0.8)
            
            record_performance(
                monitor_type="embedding",
                operation=f"æ–‡æœ¬å‘é‡åŒ–_{document_uuid[:8]}",
                duration=embedding_duration,
                token_count=token_count,  # ğŸ”¥ ä¼ å…¥ token_countï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è®¡ç®— tokens/s å’Œ ms/10k tokens
                document_uuid=document_uuid,
                chunks_count=len(chunks),
                vectors_count=len(embeddings),
                text_length=text_length,
                source="text_upload"  # æ ‡è®°ä¸ºæ–‡æœ¬ä¸Šä¼ 
            )
            
            # 3. å‡†å¤‡ Milvus å…ƒæ•°æ®
            metadata_list = []
            for i, chunk in enumerate(chunks):
                metadata_list.append({
                    "document_uuid": document_uuid,
                    "chunk_index": i,
                    "chunk_count": len(chunks),
                    **chunk["metadata"]
                })
            
            # 4. å­˜å‚¨åˆ° Milvus
            if collection_name is None:
                collection_name = MILVUS_COLLECTION_NAME
            
            # ç¡®ä¿ collection å­˜åœ¨
            existing_collections = milvus_client.list_collections()
            if collection_name not in existing_collections:
                dimension = self.milvus_config.get('dimension', 1024)
                logger.info(f"åˆ›å»º Milvus collection: {collection_name}, ç»´åº¦: {dimension}")
                milvus_client.create_collection(
                    collection_name=collection_name,
                    dimension=dimension,
                    description="æ–‡æ¡£å‘é‡å­˜å‚¨",
                    metric_type="COSINE"
                )
            
            # è½¬æ¢ embeddings ä¸ºåˆ—è¡¨
            embeddings_list = [emb.tolist() for emb in embeddings]
            
            ids = milvus_client.insert_vectors(
                collection_name=collection_name,
                embeddings=embeddings_list,
                texts=texts,
                metadata=metadata_list
            )
            success = ids is not None and len(ids) > 0
            
            if success:
                logger.info(f"âœ… æ–‡æœ¬å¤„ç†å®Œæˆ, UUID: {document_uuid}, å—æ•°: {len(chunks)}")
                return {
                    "success": True,
                    "message": "å¤„ç†æˆåŠŸ",
                    "chunks_count": len(chunks),
                    "vectors_count": len(embeddings)
                }
            else:
                return {
                    "success": False,
                    "message": "å­˜å‚¨åˆ° Milvus å¤±è´¥"
                }
                
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æœ¬å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "message": f"å¤„ç†å¼‚å¸¸: {str(e)}"
            }
    
    # ==================== å¼‚æ­¥ä»»åŠ¡æ–¹æ³• ====================
    
    def submit_task(self, task: Dict[str, Any]) -> bool:
        """
        æäº¤æ–‡æ¡£å¤„ç†ä»»åŠ¡åˆ°æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆå¼‚æ­¥ï¼‰
        
        Args:
            task: ä»»åŠ¡å­—å…¸ï¼ŒåŒ…å«:
                - task_type: ä»»åŠ¡ç±»å‹ (file, text, delete, batch)
                - document_uuid: æ–‡æ¡£ UUID
                - file_path/content: æ–‡ä»¶è·¯å¾„æˆ–æ–‡æœ¬å†…å®¹
                - metadata: å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            bool: æ˜¯å¦æäº¤æˆåŠŸ
        """
        try:
            # éªŒè¯ä»»åŠ¡
            if 'task_type' not in task:
                logger.error("ä»»åŠ¡ç¼ºå°‘ task_type å­—æ®µ")
                return False
            
            task_type = task['task_type']
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ä¸»é¢˜ï¼ˆä»… Kafka æ¨¡å¼éœ€è¦ï¼‰
            topic = None
            if message_client.mode == 'kafka':
                topics = config.get('kafka.topics', {})
                topic_map = {
                    'file': topics.get('document_embedding', 'document_embedding'),
                    'text': topics.get('document_embedding', 'document_embedding'),
                    'delete': topics.get('document_delete', 'document_delete'),
                    'batch': topics.get('batch_processing', 'batch_processing')
                }
                topic = topic_map.get(task_type)
            
            # å‘é€æ¶ˆæ¯
            success = self.message_client.send_message(
                message=task,
                topic=topic
            )
            
            if success:
                logger.info(f"æ–‡æ¡£ä»»åŠ¡å·²æäº¤: {task_type}, UUID={task.get('document_uuid')}")
            else:
                logger.error(f"æ–‡æ¡£ä»»åŠ¡æäº¤å¤±è´¥: {task_type}")
            
            return success
            
        except Exception as e:
            logger.error(f"æäº¤ä»»åŠ¡å¼‚å¸¸: {e}", exc_info=True)
            return False
    
    def start_processing(self):
        """å¯åŠ¨æ–‡æ¡£å¤„ç†æ¶ˆè´¹è€…"""
        logger.info("å¯åŠ¨æ–‡æ¡£å¤„ç†æœåŠ¡...")
        
        # å¯åŠ¨ä¸åŒç±»å‹çš„æ¶ˆè´¹è€…
        if message_client.mode == 'channel':
            # Channel æ¨¡å¼ï¼šå•ä¸ªæ¶ˆè´¹è€…å¤„ç†æ‰€æœ‰ä»»åŠ¡
            self.message_client.start_consumer(
                handler=self._process_task
            )
        else:
            # Kafka æ¨¡å¼ï¼šä¸ºæ¯ä¸ªä¸»é¢˜å¯åŠ¨æ¶ˆè´¹è€…
            topics = config.get('kafka.topics', {})
            
            # Embedding ä»»åŠ¡æ¶ˆè´¹è€…
            embedding_topic = topics.get('document_embedding', 'document_embedding')
            self.message_client.start_consumer(
                handler=self._process_task,
                topic=embedding_topic
            )
            
            # åˆ é™¤ä»»åŠ¡æ¶ˆè´¹è€…ï¼ˆå¯é€‰ï¼‰
            # delete_topic = topics.get('document_delete', 'document_delete')
            # self.message_client.start_consumer(
            #     handler=self._process_delete_task,
            #     topic=delete_topic
            # )
        
        logger.info("æ–‡æ¡£å¤„ç†æœåŠ¡å·²å¯åŠ¨")
    
    def _update_document_status_sync(
        self, 
        document_uuid: str, 
        status: int,
        page: Optional[int] = None,
        content: Optional[str] = None,
        extra_data_update: Optional[Dict[str, Any]] = None
    ):
        """
        åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­æ›´æ–°æ–‡æ¡£çŠ¶æ€ï¼ˆä¾› Kafka æ¶ˆè´¹è€…ä½¿ç”¨ï¼‰
        ç›´æ¥è°ƒç”¨åŒæ­¥ç‰ˆæœ¬ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
        
        Args:
            document_uuid: æ–‡æ¡£UUID
            status: çŠ¶æ€ç 
            page: æ–‡æ¡£é¡µæ•°ï¼ˆå¯é€‰ï¼‰
            content: æ–‡æ¡£å†…å®¹ï¼ˆå¯é€‰ï¼‰
            extra_data_update: é¢å¤–æ•°æ®æ›´æ–°ï¼ˆå¯é€‰ï¼‰
        """
        from internal.service.orm.document_sever import document_service
        
        # ç›´æ¥è°ƒç”¨åŒæ­¥ç‰ˆæœ¬ï¼Œä½¿ç”¨ pymongo è€Œä¸æ˜¯ beanie
        return document_service.update_document_status_sync(
            document_uuid, 
            status, 
            page, 
            content,
            extra_data_update  # ğŸ”¥ ä¼ é€’ extra_data_update å‚æ•°
        )
    
    def _process_task(self, task: Dict[str, Any]):
        """
        å¤„ç†æ–‡æ¡£ä»»åŠ¡ï¼ˆåˆ†å‘å™¨ï¼‰
        
        Args:
            task: ä»»åŠ¡å­—å…¸
        """
        task_type = task.get('task_type')
        
        try:
            if task_type == 'file':
                self._process_file_task(task)
            elif task_type == 'text':
                self._process_text_task(task)
            elif task_type == 'delete':
                self._process_delete_task(task)
            elif task_type == 'batch':
                self._process_batch_task(task)
            else:
                logger.warning(f"æœªçŸ¥ä»»åŠ¡ç±»å‹: {task_type}")
                
        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡å¤±è´¥: {task_type}, é”™è¯¯: {e}", exc_info=True)
    
    def _process_file_task(self, task: Dict[str, Any]):
        """å¤„ç†æ–‡ä»¶ä»»åŠ¡"""
        file_path = task.get('file_path')
        document_uuid = task.get('document_uuid')
        collection_name = task.get('collection_name')
        metadata = task.get('metadata', {})
        permission = task.get('permission', 0)  # ğŸ”¥ è·å–æƒé™ä¿¡æ¯
        
        if not file_path or not document_uuid:
            logger.error("æ–‡ä»¶ä»»åŠ¡ç¼ºå°‘å¿…è¦å­—æ®µ: file_path, document_uuid")
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†å¤±è´¥
            try:
                self._update_document_status_sync(document_uuid, 3)
            except Exception as e:
                logger.error(f"æ›´æ–°æ–‡æ¡£çŠ¶æ€å¤±è´¥: {e}")
            return
        
        result = self.process_file(
            file_path=file_path,
            document_uuid=document_uuid,
            collection_name=collection_name,
            extra_metadata=metadata,
            permission=permission  # ğŸ”¥ ä¼ é€’æƒé™ä¿¡æ¯
        )
        
        # æ ¹æ®å¤„ç†ç»“æœæ›´æ–°æ–‡æ¡£çŠ¶æ€
        try:
            if result['success']:
                # å¤„ç†æˆåŠŸï¼šstatus=2ï¼ˆå¤„ç†å®Œæˆï¼‰
                chunks_count = result.get('chunks_count', 0)
                
                # ğŸ”¥ å‡†å¤‡extra_dataæ›´æ–°ï¼ˆè®°å½•å¤„ç†æ—¶é—´ï¼‰
                extra_data_update = {
                    "embedding_time_seconds": result.get('embedding_time'),
                    "processing_time_seconds": result.get('processing_time'),
                    "processing_start_time": result.get('start_datetime'),
                    "processing_complete_time": result.get('complete_datetime'),
                    "vectors_count": result.get('vectors_count'),
                    "chunks_count": chunks_count
                }
                
                self._update_document_status_sync(
                    document_uuid, 
                    status=2,
                    page=chunks_count,  # å°† chunks_count å­˜å‚¨åˆ° page å­—æ®µ
                    extra_data_update=extra_data_update  # ğŸ”¥ æ›´æ–°extra_data
                )
                logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆï¼ŒçŠ¶æ€å·²æ›´æ–°: {document_uuid}")
            else:
                # å¤„ç†å¤±è´¥ï¼šstatus=3ï¼ˆå¤„ç†å¤±è´¥ï¼‰
                self._update_document_status_sync(document_uuid, 3)
                logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {result['message']}, çŠ¶æ€å·²æ›´æ–°: {document_uuid}")
        except Exception as e:
            logger.error(f"æ›´æ–°æ–‡æ¡£çŠ¶æ€æ—¶å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
    
    def _process_text_task(self, task: Dict[str, Any]):
        """å¤„ç†æ–‡æœ¬ä»»åŠ¡"""
        content = task.get('content')
        document_uuid = task.get('document_uuid')
        collection_name = task.get('collection_name')
        metadata = task.get('metadata', {})
        
        if not content or not document_uuid:
            logger.error("æ–‡æœ¬ä»»åŠ¡ç¼ºå°‘å¿…è¦å­—æ®µ: content, document_uuid")
            return
        
        result = self.process_text(
            text=content,
            document_uuid=document_uuid,
            collection_name=collection_name,
            metadata=metadata
        )
        
        if not result['success']:
            logger.error(f"æ–‡æœ¬å¤„ç†å¤±è´¥: {result['message']}")
    
    def _process_delete_task(self, task: Dict[str, Any]):
        """
        å¤„ç†åˆ é™¤ä»»åŠ¡
        
        Args:
            task: ä»»åŠ¡å­—å…¸ï¼ŒåŒ…å« document_id
        """
        document_id = task.get('document_id')
        
        if not document_id:
            logger.error("åˆ é™¤ä»»åŠ¡ç¼ºå°‘ document_id")
            return
        
        logger.info(f"åˆ é™¤æ–‡æ¡£: {document_id}")
        
        try:
            collection_name = MILVUS_COLLECTION_NAME
            success = milvus_client.delete_by_ids(
                collection_name=collection_name,
                ids=[document_id]
            )
            
            if success:
                logger.info(f"æ–‡æ¡£å·²ä» Milvus åˆ é™¤: {document_id}")
            else:
                logger.error(f"ä» Milvus åˆ é™¤å¤±è´¥: {document_id}")
                
        except Exception as e:
            logger.error(f"åˆ é™¤ä»»åŠ¡å¼‚å¸¸: {document_id}, é”™è¯¯: {e}", exc_info=True)
    
    def _process_batch_task(self, task: Dict[str, Any]):
        """
        å¤„ç†æ‰¹é‡ä»»åŠ¡
        
        Args:
            task: ä»»åŠ¡å­—å…¸ï¼ŒåŒ…å« tasks åˆ—è¡¨
        """
        tasks = task.get('tasks', [])
        
        if not tasks:
            logger.error("æ‰¹é‡ä»»åŠ¡ç¼ºå°‘ tasks")
            return
        
        logger.info(f"å¤„ç†æ‰¹é‡ä»»åŠ¡ï¼Œä»»åŠ¡æ•°: {len(tasks)}")
        
        # æ‰¹é‡å¤„ç†
        for sub_task in tasks:
            self._process_task(sub_task)
    
    def stop(self):
        """åœæ­¢æ–‡æ¡£å¤„ç†æœåŠ¡"""
        logger.info("æ­£åœ¨åœæ­¢æ–‡æ¡£å¤„ç†æœåŠ¡...")
        self.message_client.stop()
        logger.info("æ–‡æ¡£å¤„ç†æœåŠ¡å·²åœæ­¢")


# åˆ›å»ºå…¨å±€å®ä¾‹
document_processor = DocumentProcessor()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=" * 80)
    print("æ–‡æ¡£å¤„ç†æœåŠ¡æµ‹è¯•")
    print("=" * 80)
    
    # æäº¤æµ‹è¯•ä»»åŠ¡
    test_task = {
        "task_type": "embedding",
        "document_id": "test_doc_001",
        "content": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯ Embedding å’Œ Milvus å­˜å‚¨åŠŸèƒ½",
        "metadata": {
            "title": "æµ‹è¯•æ–‡æ¡£",
            "author": "ç™½æ€»"
        }
    }
    
    success = document_processor.submit_task(test_task)
    print(f"\nä»»åŠ¡æäº¤ç»“æœ: {success}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = message_client.get_stats()
    print(f"\nç»Ÿè®¡ä¿¡æ¯: {stats}")

