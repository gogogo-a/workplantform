"""
LLM æœåŠ¡å±‚
ç²¾ç®€ç‰ˆ - åªä¿ç•™æ ¸å¿ƒ chat åŠŸèƒ½
æ”¯æŒæç¤ºè¯å’Œå·¥å…·é…å¯¹
æ”¯æŒå¼‚æ­¥å¯¹è¯å†å²æ€»ç»“
"""
from typing import Optional, List, Dict, Any, Callable
import asyncio
from pkg.model_list import ModelManager, LLAMA_3_2  # é»˜è®¤æ¨¡å‹é…ç½®
from pkg.agent_prompt.prompt_templates import get_prompt, SUMMARY_PROMPT
from pkg.agent_tools import (
    get_tools_info,
    get_prompt_for_tools
)
from pkg.constants.constants import MAX_TOKEN


class LLMService:
    """LLM æœåŠ¡ç±» - ç²¾ç®€ç‰ˆ"""
    
    def __init__(
        self,
        model_name: Optional[str] = None,
        model_type: Optional[str] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Callable]] = None,
        auto_summary: bool = True,
        max_history_count: int = 10,
        max_history_tokens: int = MAX_TOKEN
    ):
        """
        åˆå§‹åŒ– LLM æœåŠ¡
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ LLAMA_3_2
            model_type: æ¨¡å‹ç±»å‹ (local/cloud)ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹çš„ç±»å‹
            system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            tools: ä½¿ç”¨çš„å·¥å…·å‡½æ•°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ï¼Œå¯ä»¥ç›´æ¥ç‚¹å‡»è·³è½¬
                   ä¾‹å¦‚: [knowledge_search, document_analyzer]
            auto_summary: æ˜¯å¦è‡ªåŠ¨æ€»ç»“å†å²è®°å½•
            max_history_count: å†å²è®°å½•æœ€å¤§æ¡æ•°ï¼ˆé»˜è®¤10æ¡ï¼‰
            max_history_tokens: å†å²è®°å½•æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        """
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        if model_name is None:
            model_name = LLAMA_3_2.name
            model_type = LLAMA_3_2.model_type
        elif model_type is None:
            # å¦‚æœåªæŒ‡å®šäº† model_nameï¼Œä»é…ç½®ä¸­è·å– model_type
            from pkg.model_list import get_llm_model
            config = get_llm_model(model_name)
            model_type = config.model_type
        
        self.model_name = model_name
        self.model_type = model_type
        
        # å·¥å…·é…ç½®
        self.tools = tools or []
        
        # è®¾ç½®ç³»ç»Ÿæç¤ºè¯
        if system_prompt:
            self.system_prompt = system_prompt
        elif self.tools:
            # å¦‚æœæœ‰å·¥å…·ä½†æ²¡æœ‰è‡ªå®šä¹‰æç¤ºè¯ï¼Œæ ¹æ®å·¥å…·è‡ªåŠ¨é€‰æ‹©
            prompt_template = get_prompt_for_tools(self.tools)
            self.system_prompt = get_prompt(prompt_template)
        else:
            self.system_prompt = get_prompt("default")
        
        # å†å²è®°å½•ç®¡ç†
        self.chat_history: List[Dict[str, str]] = []
        self.auto_summary = auto_summary
        self.max_history_count = max_history_count
        self.max_history_tokens = max_history_tokens
        self._is_summarizing = False  # æ€»ç»“çŠ¶æ€æ ‡å¿—
        self._need_summary = False  # æ˜¯å¦éœ€è¦åœ¨ä¸‹æ¬¡å¯¹è¯å‰æ€»ç»“
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.llm = None
        self._initialize()
    
    def _initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            self.llm = ModelManager.select_llm_model(self.model_name, self.model_type)
            print(f"âœ“ æ¨¡å‹å·²åŠ è½½: {self.model_name} (type: {self.model_type})")
            if self.tools:
                tool_names = [t.__name__ for t in self.tools]
                print(f"âœ“ å·²å¯ç”¨å·¥å…·: {tool_names}")
        except Exception as e:
            print(f"âœ— æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def chat(
        self,
        user_message: Optional[str] = None,
        messages: Optional[List[Dict[str, str]]] = None,
        context: Optional[str] = None,
        stream: bool = True,
        use_history: bool = True,
        **kwargs
    ):
        """
        å¯¹è¯æ–¹æ³•
        
        ğŸ”¥ ä¼˜åŒ–ï¼šè‡ªåŠ¨ä½¿ç”¨å†…éƒ¨å†å²è®°å½•ï¼Œæ— éœ€æ‰‹åŠ¨è·å–
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯ï¼ˆç®€åŒ–ç”¨æ³•ï¼‰
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆé«˜çº§ç”¨æ³•ï¼Œä¼ å…¥åˆ™å¿½ç•¥ user_messageï¼‰
                     - å¦‚æœä¸åŒ…å«å†å²ï¼Œä¼šè‡ªåŠ¨æ·»åŠ 
            context: é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚çŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼‰
            stream: æ˜¯å¦æµå¼è¿”å›
            use_history: æ˜¯å¦è‡ªåŠ¨ä½¿ç”¨å†…éƒ¨å†å²è®°å½•ï¼ˆé»˜è®¤ Trueï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        
        Yields (stream=True):
            å›å¤ç‰‡æ®µ
            
        Returns (stream=False):
            å®Œæ•´å›å¤
            
        ç¤ºä¾‹:
            # ç®€åŒ–ç”¨æ³•ï¼ˆæ¨èï¼‰
            llm.chat("ä½ å¥½")
            
            # é«˜çº§ç”¨æ³•ï¼ˆå®Œå…¨æ§åˆ¶ï¼‰
            llm.chat(messages=[{"role": "user", "content": "ä½ å¥½"}], use_history=False)
        
        Note:
            å·¥å…·è°ƒç”¨ç”± Agent å±‚å¤„ç†ï¼Œä¸åœ¨æ­¤æ–¹æ³•ä¸­æ§åˆ¶
        """
        # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šåœ¨ AI å›ç­”å‰ï¼Œå¦‚æœéœ€è¦æ€»ç»“ï¼Œå…ˆæ‰§è¡Œæ€»ç»“
        if self._need_summary and not self._is_summarizing:
            print("\nâš¡ æ£€æµ‹åˆ°éœ€è¦æ€»ç»“å†å²è®°å½•ï¼Œæ­£åœ¨æ€»ç»“...")
            self.summarize_history()
            self._need_summary = False
        
        # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
        full_messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if self.system_prompt:
            full_messages.append({
                "role": "system",
                "content": self.system_prompt
            })
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ï¼ˆå¦‚çŸ¥è¯†åº“å†…å®¹ï¼‰
        if context:
            full_messages.append({
                "role": "system",
                "content": f"å‚è€ƒä¿¡æ¯ï¼š\n{context}"
            })
        
        # ğŸ”¥ è‡ªåŠ¨æ·»åŠ å†å²è®°å½•
        if use_history and self.chat_history:
            full_messages.extend(self.chat_history)
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        if user_message and messages:
            # âŒ ä¸å…è®¸åŒæ—¶æä¾›ä¸¤ä¸ªå‚æ•°ï¼Œé¿å…æ··æ·†
            raise ValueError("ä¸èƒ½åŒæ—¶æä¾› user_message å’Œ messages å‚æ•°ï¼Œè¯·é€‰æ‹©å…¶ä¸€")
        elif user_message:
            # ç®€åŒ–ç”¨æ³•ï¼šç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²
            full_messages.append({
                "role": "user",
                "content": user_message
            })
        elif messages:
            # é«˜çº§ç”¨æ³•ï¼šä¼ å…¥å®Œæ•´æ¶ˆæ¯åˆ—è¡¨
            full_messages.extend(messages)
        else:
            raise ValueError("å¿…é¡»æä¾› user_message æˆ– messages å‚æ•°")
        
        # æ ¼å¼åŒ–ä¸º prompt
        prompt = self._format_messages(full_messages)
        
        # ç”Ÿæˆå›å¤
        if stream:
            return self._stream_generate(prompt, **kwargs)
        else:
            return self._generate(prompt, **kwargs)
    
    def _normalize_chunk(self, chunk) -> str:
        """
        æ ‡å‡†åŒ–ä¸åŒæ¨¡å‹è¿”å›çš„ chunk æ ¼å¼
        
        Args:
            chunk: åŸå§‹ chunkï¼ˆå¯èƒ½æ˜¯ str æˆ– AIMessageChunkï¼‰
            
        Returns:
            æ ‡å‡†åŒ–åçš„å­—ç¬¦ä¸²
        """
        # Ollama è¿”å›å­—ç¬¦ä¸²ï¼ŒChatOpenAI è¿”å› AIMessageChunk
        if isinstance(chunk, str):
            return chunk
        else:
            # AIMessageChunk å¯¹è±¡ï¼Œæå– content
            return chunk.content if hasattr(chunk, 'content') else str(chunk)
    
    def _stream_generate(self, prompt: str, **kwargs):
        """æµå¼ç”Ÿæˆ"""
        try:
            for chunk in self.llm.stream(prompt):
                yield self._normalize_chunk(chunk)
        except Exception as e:
            raise Exception(f"ç”Ÿæˆå¤±è´¥: {e}")
    
    def _generate(self, prompt: str, **kwargs) -> str:
        """éæµå¼ç”Ÿæˆ"""
        try:
            chunks = [self._normalize_chunk(chunk) for chunk in self.llm.stream(prompt)]
            return "".join(chunks)
        except Exception as e:
            raise Exception(f"ç”Ÿæˆå¤±è´¥: {e}")
    
    def _format_messages(self, messages: List[Dict[str, str]]) -> str:
        """æ ¼å¼åŒ–æ¶ˆæ¯ä¸º prompt"""
        formatted = []
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                formatted.append(f"System: {content}")
            elif role == "user":
                formatted.append(f"User: {content}")
            elif role == "assistant":
                formatted.append(f"Assistant: {content}")
        
        formatted.append("Assistant:")
        return "\n\n".join(formatted)
    
    def _estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡
        ç®€å•ä¼°ç®—ï¼šä¸­æ–‡1å­—çº¦1.5tokenï¼Œè‹±æ–‡1è¯çº¦1token
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            ä¼°ç®—çš„ token æ•°
        """
        # ç®€å•ä¼°ç®—æ–¹æ³•
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        other_chars = len(text) - chinese_chars
        return int(chinese_chars * 1.5 + other_chars / 4)
    
    def _calculate_history_tokens(self) -> int:
        """è®¡ç®—å†å²è®°å½•çš„æ€» token æ•°"""
        total_tokens = 0
        for msg in self.chat_history:
            content = msg.get("content", "")
            total_tokens += self._estimate_tokens(content)
        return total_tokens
    
    def _should_summarize(self) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦æ€»ç»“å†å²è®°å½•
        
        Returns:
            æ˜¯å¦éœ€è¦æ€»ç»“
        """
        # å¦‚æœæ­£åœ¨æ€»ç»“ä¸­ï¼Œä¸å†è§¦å‘
        if self._is_summarizing:
            return False
        
        # æ£€æŸ¥æ¡æ•°
        if len(self.chat_history) >= self.max_history_count:
            return True
        
        # æ£€æŸ¥ token æ•°
        total_tokens = self._calculate_history_tokens()
        if total_tokens >= self.max_history_tokens:
            return True
        
        return False
    
    def _do_summarize(self) -> str:
        """
        æ‰§è¡Œæ€»ç»“çš„æ ¸å¿ƒé€»è¾‘ï¼ˆæå–å…¬å…±ä»£ç ï¼‰
        
        Returns:
            æ€»ç»“å†…å®¹
        """
        # æ„å»ºæ€»ç»“ prompt
        history_text = "\n\n".join([
            f"{msg['role']}: {msg['content']}"
            for msg in self.chat_history
        ])
        
        summary_messages = [
            {"role": "system", "content": SUMMARY_PROMPT},
            {"role": "user", "content": f"è¯·æ€»ç»“ä»¥ä¸‹å¯¹è¯ï¼š\n\n{history_text}"}
        ]
        
        # æ ¼å¼åŒ–å¹¶ç”Ÿæˆæ€»ç»“ï¼ˆä½¿ç”¨ _normalize_chunk ç»Ÿä¸€å¤„ç†ï¼‰
        prompt = self._format_messages(summary_messages)
        summary_chunks = [
            self._normalize_chunk(chunk) 
            for chunk in self.llm.stream(prompt)
        ]
        
        return "".join(summary_chunks)
    
    async def _summarize_history_async(self):
        """
        å¼‚æ­¥æ€»ç»“å†å²è®°å½•
        åœ¨åå°è¿è¡Œï¼Œä¸é˜»å¡å½“å‰å¯¹è¯
        """
        if not self.chat_history or self._is_summarizing:
            return
        
        self._is_summarizing = True
        
        try:
            old_count = len(self.chat_history)
            summary = self._do_summarize()
            
            # æ›¿æ¢å†å²è®°å½•ä¸ºæ€»ç»“
            self.chat_history = [
                {
                    "role": "assistant",
                    "content": f"[å†å²å¯¹è¯æ€»ç»“] {summary}"
                }
            ]
            
            print(f"\nâœ“ å†å²è®°å½•å·²æ€»ç»“ï¼ˆåŸ {old_count} æ¡ -> 1 æ¡ï¼‰")
            
        except Exception as e:
            print(f"âœ— æ€»ç»“å†å²è®°å½•å¤±è´¥: {e}")
        finally:
            self._is_summarizing = False
    
    def summarize_history(self):
        """
        åŒæ­¥æ–¹å¼æ€»ç»“å†å²è®°å½•
        é€‚ç”¨äºåŒæ­¥ä¸Šä¸‹æ–‡ï¼ˆéå¼‚æ­¥ç¯å¢ƒï¼‰
        """
        if not self.chat_history or self._is_summarizing:
            return
        
        self._is_summarizing = True
        
        try:
            old_count = len(self.chat_history)
            summary = self._do_summarize()
            
            # æ›¿æ¢å†å²è®°å½•ä¸ºæ€»ç»“
            self.chat_history = [
                {
                    "role": "assistant",
                    "content": f"[å†å²å¯¹è¯æ€»ç»“] {summary}"
                }
            ]
            
            print(f"\nâœ“ å†å²è®°å½•å·²æ€»ç»“ï¼ˆåŸ {old_count} æ¡ -> 1 æ¡ï¼‰")
            
        except Exception as e:
            print(f"âœ— æ€»ç»“å†å²è®°å½•å¤±è´¥: {e}")
        finally:
            self._is_summarizing = False
    
    def add_to_history(self, role: str, content: str):
        """
        æ·»åŠ æ¶ˆæ¯åˆ°å†å²è®°å½•
        
        ğŸ”¥ ä¼˜åŒ–ï¼šåªæ£€æŸ¥æ˜¯å¦éœ€è¦æ€»ç»“ï¼Œä¸ç«‹å³æ‰§è¡Œ
        æ€»ç»“ä¼šåœ¨ä¸‹æ¬¡ chat() è°ƒç”¨æ—¶ï¼ˆAIå›ç­”å‰ï¼‰è‡ªåŠ¨æ‰§è¡Œ
        
        Args:
            role: è§’è‰² (user/assistant/system)
            content: å†…å®¹
        """
        self.chat_history.append({
            "role": role,
            "content": content
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ€»ç»“ï¼ˆåªæ ‡è®°ï¼Œä¸æ‰§è¡Œï¼‰
        # åªåœ¨é¦–æ¬¡è¶…é™æ—¶æ‰“å°æç¤ºï¼Œé¿å…é‡å¤
        if self.auto_summary and self._should_summarize() and not self._need_summary:
            self._need_summary = True
            print(f"ğŸ“Œ å†å²è®°å½•å·²è¾¾åˆ°é™åˆ¶ï¼ˆ{len(self.chat_history)}æ¡ï¼‰ï¼Œå°†åœ¨ä¸‹æ¬¡å¯¹è¯å‰è‡ªåŠ¨æ€»ç»“")
    
    def get_history(self) -> List[Dict[str, str]]:
        """è·å–å½“å‰å†å²è®°å½•"""
        return self.chat_history.copy()
    
    def clear_history(self):
        """æ¸…ç©ºå†å²è®°å½•"""
        self.chat_history = []
        print("âœ“ å†å²è®°å½•å·²æ¸…ç©º")
    
    def get_history_stats(self) -> Dict[str, Any]:
        """è·å–å†å²è®°å½•ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "count": len(self.chat_history),
            "total_tokens": self._calculate_history_tokens(),
            "max_count": self.max_history_count,
            "max_tokens": self.max_history_tokens,
            "is_summarizing": self._is_summarizing
        }
    
    def get_info(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡ä¿¡æ¯"""
        tool_info = get_tools_info(self.tools) if self.tools else []
        return {
            "model_name": self.model_name,
            "model_type": self.model_type,
            "system_prompt": self.system_prompt[:100] + "..." if len(self.system_prompt) > 100 else self.system_prompt,
            "tools": [t["name"] for t in tool_info],
            "tool_count": len(self.tools),
            "auto_summary": self.auto_summary,
            "history_count": len(self.chat_history)
        }
