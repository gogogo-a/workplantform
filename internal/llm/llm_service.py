"""
LLM æœåŠ¡å±‚ - LangChain ç‰ˆæœ¬
ä½¿ç”¨ LangChain æ ‡å‡†ç»„ä»¶ï¼š
- ChatMessageHistory: å†å²è®°å½•ç®¡ç†
- SystemMessage/HumanMessage/AIMessage: æ¶ˆæ¯ç±»å‹
- æ”¯æŒè‡ªåŠ¨æ€»ç»“å†å²è®°å½•
"""
from typing import Optional, List, Dict, Any, Callable
import asyncio
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from pkg.model_list import ModelManager, LLAMA_3_2  # é»˜è®¤æ¨¡å‹é…ç½®
from pkg.agent_prompt.prompt_templates import get_prompt, SUMMARY_PROMPT
from pkg.agent_tools import (
    get_tools_info,
    get_prompt_for_tools
)
from pkg.constants.constants import MAX_TOKEN


class LLMService:
    """LLM æœåŠ¡ç±» - ç²¾ç®€ç‰ˆ"""
    
    def __init__(
        self,
        model_name: Optional[str] = None,
        model_type: Optional[str] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Callable]] = None,
        auto_summary: bool = True,
        max_history_count: int = 10,
        max_history_tokens: int = MAX_TOKEN
    ):
        """
        åˆå§‹åŒ– LLM æœåŠ¡
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ LLAMA_3_2
            model_type: æ¨¡å‹ç±»å‹ (local/cloud)ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹çš„ç±»å‹
            system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            tools: ä½¿ç”¨çš„å·¥å…·å‡½æ•°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ï¼Œå¯ä»¥ç›´æ¥ç‚¹å‡»è·³è½¬
                   ä¾‹å¦‚: [knowledge_search, document_analyzer]
            auto_summary: æ˜¯å¦è‡ªåŠ¨æ€»ç»“å†å²è®°å½•
            max_history_count: å†å²è®°å½•æœ€å¤§æ¡æ•°ï¼ˆé»˜è®¤10æ¡ï¼‰
            max_history_tokens: å†å²è®°å½•æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        """
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        if model_name is None:
            model_name = LLAMA_3_2.name
            model_type = LLAMA_3_2.model_type
        elif model_type is None:
            # å¦‚æœåªæŒ‡å®šäº† model_nameï¼Œä»é…ç½®ä¸­è·å– model_type
            from pkg.model_list import get_llm_model
            config = get_llm_model(model_name)
            model_type = config.model_type
        
        self.model_name = model_name
        self.model_type = model_type
        
        # å·¥å…·é…ç½®
        self.tools = tools or []
        
        # è®¾ç½®ç³»ç»Ÿæç¤ºè¯
        if system_prompt:
            self.system_prompt = system_prompt
        elif self.tools:
            # å¦‚æœæœ‰å·¥å…·ä½†æ²¡æœ‰è‡ªå®šä¹‰æç¤ºè¯ï¼Œæ ¹æ®å·¥å…·è‡ªåŠ¨é€‰æ‹©
            prompt_template = get_prompt_for_tools(self.tools)
            self.system_prompt = get_prompt(prompt_template)
        else:
            self.system_prompt = get_prompt("default")
        
        # ğŸ”¥ ä½¿ç”¨ LangChain çš„å†å²è®°å½•ç®¡ç†
        self.chat_history: BaseChatMessageHistory = InMemoryChatMessageHistory()
        self.auto_summary = auto_summary
        self.max_history_count = max_history_count
        self.max_history_tokens = max_history_tokens
        self._is_summarizing = False  # æ€»ç»“çŠ¶æ€æ ‡å¿—
        self._need_summary = False  # æ˜¯å¦éœ€è¦åœ¨ä¸‹æ¬¡å¯¹è¯å‰æ€»ç»“
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.llm = None
        self._initialize()
    
    def _initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            self.llm = ModelManager.select_llm_model(self.model_name, self.model_type)
            print(f"âœ“ æ¨¡å‹å·²åŠ è½½: {self.model_name} (type: {self.model_type})")
            if self.tools:
                # å…¼å®¹ LangChain Tool å¯¹è±¡å’Œæ™®é€šå‡½æ•°
                from langchain_core.tools import Tool
                tool_names = []
                for t in self.tools:
                    if isinstance(t, Tool):
                        tool_names.append(t.name)
                    else:
                        tool_names.append(t.__name__)
                print(f"âœ“ å·²å¯ç”¨å·¥å…·: {tool_names}")
        except Exception as e:
            print(f"âœ— æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def chat(
        self,
        user_message: Optional[str] = None,
        messages: Optional[List[Dict[str, str]]] = None,
        context: Optional[str] = None,
        stream: bool = True,
        use_history: bool = True,
        **kwargs
    ):
        """
        å¯¹è¯æ–¹æ³• - LangChain ç‰ˆæœ¬
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯ï¼ˆç®€åŒ–ç”¨æ³•ï¼‰
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆé«˜çº§ç”¨æ³•ï¼‰
            context: é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚çŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼‰
            stream: æ˜¯å¦æµå¼è¿”å›
            use_history: æ˜¯å¦è‡ªåŠ¨ä½¿ç”¨å†…éƒ¨å†å²è®°å½•ï¼ˆé»˜è®¤ Trueï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        
        Yields (stream=True):
            å›å¤ç‰‡æ®µ
            
        Returns (stream=False):
            å®Œæ•´å›å¤
        """
        # ğŸ”¥ åœ¨ AI å›ç­”å‰ï¼Œå¦‚æœéœ€è¦æ€»ç»“ï¼Œå…ˆæ‰§è¡Œæ€»ç»“
        if self._need_summary and not self._is_summarizing:
            print("\nâš¡ æ£€æµ‹åˆ°éœ€è¦æ€»ç»“å†å²è®°å½•ï¼Œæ­£åœ¨æ€»ç»“...")
            self.summarize_history()
            self._need_summary = False
        
        # ğŸ”¥ æ„å»º LangChain æ¶ˆæ¯åˆ—è¡¨
        langchain_messages: List[BaseMessage] = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if self.system_prompt:
            langchain_messages.append(SystemMessage(content=self.system_prompt))
        
        # æ·»åŠ ä¸Šä¸‹æ–‡
        if context:
            langchain_messages.append(SystemMessage(content=f"å‚è€ƒä¿¡æ¯ï¼š\n{context}"))
        
        # ğŸ”¥ è‡ªåŠ¨æ·»åŠ å†å²è®°å½•ï¼ˆLangChain æ ¼å¼ï¼‰
        if use_history:
            langchain_messages.extend(self.chat_history.messages)
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        if user_message and messages:
            raise ValueError("ä¸èƒ½åŒæ—¶æä¾› user_message å’Œ messages å‚æ•°")
        elif user_message:
            langchain_messages.append(HumanMessage(content=user_message))
        elif messages:
            # è½¬æ¢ä¸º LangChain æ¶ˆæ¯æ ¼å¼
            for msg in messages:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                if role == "system":
                    langchain_messages.append(SystemMessage(content=content))
                elif role == "user":
                    langchain_messages.append(HumanMessage(content=content))
                elif role == "assistant":
                    langchain_messages.append(AIMessage(content=content))
        else:
            raise ValueError("å¿…é¡»æä¾› user_message æˆ– messages å‚æ•°")
        
        # ğŸ”¥ ä½¿ç”¨ LangChain çš„ stream æ–¹æ³•
        if stream:
            return self._stream_generate(langchain_messages, **kwargs)
        else:
            return self._generate(langchain_messages, **kwargs)
    
    def _normalize_chunk(self, chunk) -> str:
        """
        æ ‡å‡†åŒ–ä¸åŒæ¨¡å‹è¿”å›çš„ chunk æ ¼å¼
        
        Args:
            chunk: åŸå§‹ chunkï¼ˆå¯èƒ½æ˜¯ str æˆ– AIMessageChunkï¼‰
            
        Returns:
            æ ‡å‡†åŒ–åçš„å­—ç¬¦ä¸²
        """
        # Ollama è¿”å›å­—ç¬¦ä¸²ï¼ŒChatOpenAI è¿”å› AIMessageChunk
        if isinstance(chunk, str):
            return chunk
        else:
            # AIMessageChunk å¯¹è±¡ï¼Œæå– content
            return chunk.content if hasattr(chunk, 'content') else str(chunk)
    
    def _stream_generate(self, messages: List[BaseMessage], **kwargs):
        """æµå¼ç”Ÿæˆ - LangChain ç‰ˆæœ¬"""
        try:
            # ğŸ”¥ ç›´æ¥ä¼ é€’ LangChain æ¶ˆæ¯åˆ—è¡¨
            for chunk in self.llm.stream(messages):
                yield self._normalize_chunk(chunk)
        except Exception as e:
            raise Exception(f"ç”Ÿæˆå¤±è´¥: {e}")
    
    def _generate(self, messages: List[BaseMessage], **kwargs) -> str:
        """éæµå¼ç”Ÿæˆ - LangChain ç‰ˆæœ¬"""
        try:
            # ğŸ”¥ ä½¿ç”¨ invoke æ–¹æ³•
            response = self.llm.invoke(messages)
            return self._normalize_chunk(response)
        except Exception as e:
            raise Exception(f"ç”Ÿæˆå¤±è´¥: {e}")
    
    def _estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡
        ç®€å•ä¼°ç®—ï¼šä¸­æ–‡1å­—çº¦1.5tokenï¼Œè‹±æ–‡1è¯çº¦1token
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            ä¼°ç®—çš„ token æ•°
        """
        # ç®€å•ä¼°ç®—æ–¹æ³•
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        other_chars = len(text) - chinese_chars
        return int(chinese_chars * 1.5 + other_chars / 4)
    
    def _calculate_history_tokens(self) -> int:
        """è®¡ç®—å†å²è®°å½•çš„æ€» token æ•° - LangChain ç‰ˆæœ¬"""
        total_tokens = 0
        for msg in self.chat_history.messages:
            content = msg.content if hasattr(msg, 'content') else str(msg)
            total_tokens += self._estimate_tokens(content)
        return total_tokens
    
    def _should_summarize(self) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦æ€»ç»“å†å²è®°å½•
        
        Returns:
            æ˜¯å¦éœ€è¦æ€»ç»“
        """
        # å¦‚æœæ­£åœ¨æ€»ç»“ä¸­ï¼Œä¸å†è§¦å‘
        if self._is_summarizing:
            return False
        
        # æ£€æŸ¥æ¡æ•°
        if len(self.chat_history) >= self.max_history_count:
            return True
        
        # æ£€æŸ¥ token æ•°
        total_tokens = self._calculate_history_tokens()
        if total_tokens >= self.max_history_tokens:
            return True
        
        return False
    
    def _do_summarize(self) -> str:
        """
        æ‰§è¡Œæ€»ç»“çš„æ ¸å¿ƒé€»è¾‘ - LangChain ç‰ˆæœ¬
        
        Returns:
            æ€»ç»“å†…å®¹
        """
        # ğŸ”¥ æ„å»ºæ€»ç»“ promptï¼ˆLangChain æ ¼å¼ï¼‰
        history_text = "\n\n".join([
            f"{msg.type}: {msg.content}"
            for msg in self.chat_history.messages
        ])
        
        summary_messages = [
            SystemMessage(content=SUMMARY_PROMPT),
            HumanMessage(content=f"è¯·æ€»ç»“ä»¥ä¸‹å¯¹è¯ï¼š\n\n{history_text}")
        ]
        
        # ç”Ÿæˆæ€»ç»“
        summary_chunks = [
            self._normalize_chunk(chunk) 
            for chunk in self.llm.stream(summary_messages)
        ]
        
        return "".join(summary_chunks)
    
    async def _summarize_history_async(self):
        """
        å¼‚æ­¥æ€»ç»“å†å²è®°å½• - LangChain ç‰ˆæœ¬
        """
        if not self.chat_history.messages or self._is_summarizing:
            return
        
        self._is_summarizing = True
        
        try:
            old_count = len(self.chat_history.messages)
            summary = self._do_summarize()
            
            # ğŸ”¥ æ¸…ç©ºå¹¶æ·»åŠ æ€»ç»“ï¼ˆLangChain æ ¼å¼ï¼‰
            self.chat_history.clear()
            self.chat_history.add_message(
                AIMessage(content=f"[å†å²å¯¹è¯æ€»ç»“] {summary}")
            )
            
            print(f"\nâœ“ å†å²è®°å½•å·²æ€»ç»“ï¼ˆåŸ {old_count} æ¡ -> 1 æ¡ï¼‰")
            
        except Exception as e:
            print(f"âœ— æ€»ç»“å†å²è®°å½•å¤±è´¥: {e}")
        finally:
            self._is_summarizing = False
    
    def summarize_history(self):
        """
        åŒæ­¥æ–¹å¼æ€»ç»“å†å²è®°å½• - LangChain ç‰ˆæœ¬
        """
        if not self.chat_history.messages or self._is_summarizing:
            return
        
        self._is_summarizing = True
        
        try:
            old_count = len(self.chat_history.messages)
            summary = self._do_summarize()
            
            # ğŸ”¥ æ¸…ç©ºå¹¶æ·»åŠ æ€»ç»“ï¼ˆLangChain æ ¼å¼ï¼‰
            self.chat_history.clear()
            self.chat_history.add_message(
                AIMessage(content=f"[å†å²å¯¹è¯æ€»ç»“] {summary}")
            )
            
            print(f"\nâœ“ å†å²è®°å½•å·²æ€»ç»“ï¼ˆåŸ {old_count} æ¡ -> 1 æ¡ï¼‰")
            
        except Exception as e:
            print(f"âœ— æ€»ç»“å†å²è®°å½•å¤±è´¥: {e}")
        finally:
            self._is_summarizing = False
    
    def add_to_history(self, role: str, content: str):
        """
        æ·»åŠ æ¶ˆæ¯åˆ°å†å²è®°å½• - LangChain ç‰ˆæœ¬
        
        Args:
            role: è§’è‰² (user/assistant/system)
            content: å†…å®¹
        """
        # ğŸ”¥ ä½¿ç”¨ LangChain çš„ add_message æ–¹æ³•
        if role == "user":
            self.chat_history.add_message(HumanMessage(content=content))
        elif role == "assistant":
            self.chat_history.add_message(AIMessage(content=content))
        elif role == "system":
            self.chat_history.add_message(SystemMessage(content=content))
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ€»ç»“
        if self.auto_summary and self._should_summarize() and not self._need_summary:
            self._need_summary = True
            print(f"ğŸ“Œ å†å²è®°å½•å·²è¾¾åˆ°é™åˆ¶ï¼ˆ{len(self.chat_history.messages)}æ¡ï¼‰ï¼Œå°†åœ¨ä¸‹æ¬¡å¯¹è¯å‰è‡ªåŠ¨æ€»ç»“")
    
    def get_history(self) -> List[BaseMessage]:
        """è·å–å½“å‰å†å²è®°å½• - LangChain ç‰ˆæœ¬"""
        return self.chat_history.messages.copy()
    
    def clear_history(self):
        """æ¸…ç©ºå†å²è®°å½• - LangChain ç‰ˆæœ¬"""
        self.chat_history.clear()
        print("âœ“ å†å²è®°å½•å·²æ¸…ç©º")
    
    def get_history_stats(self) -> Dict[str, Any]:
        """è·å–å†å²è®°å½•ç»Ÿè®¡ä¿¡æ¯ - LangChain ç‰ˆæœ¬"""
        return {
            "count": len(self.chat_history.messages),
            "total_tokens": self._calculate_history_tokens(),
            "max_count": self.max_history_count,
            "max_tokens": self.max_history_tokens,
            "is_summarizing": self._is_summarizing
        }
    
    def get_info(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡ä¿¡æ¯ - LangChain ç‰ˆæœ¬"""
        tool_info = get_tools_info(self.tools) if self.tools else []
        return {
            "model_name": self.model_name,
            "model_type": self.model_type,
            "system_prompt": self.system_prompt[:100] + "..." if len(self.system_prompt) > 100 else self.system_prompt,
            "tools": [t["name"] for t in tool_info],
            "tool_count": len(self.tools),
            "auto_summary": self.auto_summary,
            "history_count": len(self.chat_history.messages)
        }
