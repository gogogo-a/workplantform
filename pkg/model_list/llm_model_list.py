"""
LLM æ¨¡å‹åˆ—è¡¨
å®šä¹‰æ‰€æœ‰å¯ç”¨çš„å¤§è¯­è¨€æ¨¡å‹
"""
from .base_model import LLMModelConfig


# ==================== æœ¬åœ°æ¨¡å‹ ====================

LLAMA_3_2 = LLMModelConfig(
    name="llama3.2",
    model_path="llama3.2",
    description="Llama 3.2 æœ¬åœ°æ¨¡å‹",
    provider="ollama",
    model_type="local",
    temperature=0.3,
    max_tokens=2048,
    timeout=120
)


# ==================== äº‘ç«¯æ¨¡å‹ ====================

DEEPSEEK_CHAT = LLMModelConfig(
    name="deepseek-chat",
    model_path="deepseek-chat",
    description="DeepSeek Chat äº‘ç«¯æ¨¡å‹",
    provider="deepseek",
    model_type="cloud",
    temperature=0.3,
    max_tokens=4096,
    timeout=30  # ğŸ”¥ è°ƒæ•´ä¸º 30 ç§’ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
)


# ==================== æ¨¡å‹å­—å…¸ï¼ˆç”¨äºæŸ¥æ‰¾ï¼‰====================

LLM_MODELS = {
    # æœ¬åœ°æ¨¡å‹
    "llama3.2": LLAMA_3_2,
    
    # äº‘ç«¯æ¨¡å‹
    "deepseek-chat": DEEPSEEK_CHAT,
}


# ==================== è¾…åŠ©å‡½æ•° ====================

def get_llm_model(model_name: str) -> LLMModelConfig:
    """
    è·å– LLM æ¨¡å‹é…ç½®
    
    Args:
        model_name: æ¨¡å‹åç§°
        
    Returns:
        LLMModelConfig: æ¨¡å‹é…ç½®å¯¹è±¡
        
    Raises:
        ValueError: å¦‚æœæ¨¡å‹ä¸å­˜åœ¨
    """
    if model_name not in LLM_MODELS:
        available = ", ".join(LLM_MODELS.keys())
        raise ValueError(f"æœªæ‰¾åˆ° LLM æ¨¡å‹ '{model_name}'ã€‚å¯ç”¨æ¨¡å‹: {available}")
    return LLM_MODELS[model_name]


def list_llm_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ LLM æ¨¡å‹"""
    return list(LLM_MODELS.values())

