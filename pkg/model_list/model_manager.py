"""
ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨
é›†ä¸­ç®¡ç†æ‰€æœ‰ç±»å‹æ¨¡å‹çš„é€‰æ‹©ã€åˆå§‹åŒ–å’Œé…ç½®
"""
from typing import Any, Union, Optional
import logging

# ğŸ”¥ å…³é”®ï¼šå¿…é¡»åœ¨å¯¼å…¥å…¶ä»–åº“ä¹‹å‰å…ˆå¯¼å…¥ constants
# ä»¥ä¾¿è®¾ç½® HuggingFace ç¦»çº¿æ¨¡å¼ç­‰ç¯å¢ƒå˜é‡
from pkg.constants.constants import OLLAMA_BASE_URL, DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, RUNNING_MODE

from .llm_model_list import (
    LLM_MODELS, 
    LLAMA_3_2, 
    DEEPSEEK_CHAT,
    get_llm_model, 
    list_llm_models
)
from .embedding_model_list import (
    EMBEDDING_MODELS,
    BGE_LARGE_ZH_V1_5,
    BGE_BASE_ZH_V1_5,
    get_embedding_model,
    list_embedding_models
)
from .reranker_model_list import (
    RERANKER_MODELS,
    BGE_RERANKER_V2_M3,
    BGE_RERANKER_LARGE,
    get_reranker_model,
    list_reranker_models
)
from .base_model import LLMModelConfig, EmbeddingModelConfig, RerankerModelConfig

logger = logging.getLogger(__name__)


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ¨¡å‹"""
    
    @staticmethod
    def select_llm_model(model_name: str, model_type: Optional[str] = None) -> Any:
        """
        é€‰æ‹©å¹¶åˆå§‹åŒ– LLM æ¨¡å‹
        
        æ³¨æ„ï¼š
        - Ollama æœ¬åœ°æ¨¡å‹çš„ GPU/CPU ä½¿ç”¨ç”± Ollama æœåŠ¡ç«¯è‡ªåŠ¨å†³å®šï¼Œä¸ç”±å®¢æˆ·ç«¯ä»£ç æ§åˆ¶
        - Embedding å’Œ Reranker æ¨¡å‹çš„è®¾å¤‡é€šè¿‡ RUNNING_MODE ç¯å¢ƒå˜é‡é…ç½®
        
        Args:
            model_name: æ¨¡å‹åç§° (å¦‚ "llama3.2", "deepseek-chat")
            model_type: æ¨¡å‹ç±»å‹ ("local" æˆ– "cloud")ï¼Œå¯é€‰ï¼Œç”¨äºéªŒè¯
            
        Returns:
            LLM å®ä¾‹
        """
        # è·å–æ¨¡å‹é…ç½®
        config = get_llm_model(model_name)
        
        # å¦‚æœæä¾›äº† model_typeï¼ŒéªŒè¯æ˜¯å¦åŒ¹é…
        if model_type is not None and config.model_type != model_type:
            raise ValueError(
                f"æ¨¡å‹ '{model_name}' çš„ç±»å‹æ˜¯ '{config.model_type}'ï¼Œ"
                f"ä½†è¯·æ±‚çš„ç±»å‹æ˜¯ '{model_type}'"
            )
        
        # æ ¹æ®æ¨¡å‹ç±»å‹å’Œæä¾›å•†åˆå§‹åŒ–æ¨¡å‹
        if config.model_type == "local" and config.provider == "ollama":
            from langchain_community.llms import Ollama
            llm = Ollama(
                base_url=OLLAMA_BASE_URL,
                model=config.model_path,
                temperature=config.temperature,
                timeout=config.timeout,
                num_predict=config.max_tokens
            )
            logger.info(f"âœ“ å·²é€‰æ‹©æœ¬åœ°æ¨¡å‹: {model_name} (type: {config.model_type})")
            print(f"âœ“ å·²é€‰æ‹©æœ¬åœ°æ¨¡å‹: {model_name} (type: {config.model_type})")
            return llm
            
        elif config.model_type == "cloud" and config.provider == "deepseek":
            if not DEEPSEEK_API_KEY:
                raise ValueError("DEEPSEEK_API_KEY æœªé…ç½®ï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®")
            
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(
                model=config.model_path,
                openai_api_key=DEEPSEEK_API_KEY,
                openai_api_base=DEEPSEEK_BASE_URL,
                temperature=config.temperature,
                max_tokens=config.max_tokens,
                request_timeout=config.timeout  # ğŸ”¥ æ·»åŠ è¶…æ—¶è®¾ç½®ï¼ˆLangChain ä½¿ç”¨ request_timeoutï¼‰
            )
            logger.info(f"âœ“ å·²é€‰æ‹©äº‘ç«¯æ¨¡å‹: {model_name} (type: {config.model_type}, timeout: {config.timeout}s)")
            print(f"âœ“ å·²é€‰æ‹©äº‘ç«¯æ¨¡å‹: {model_name} (type: {config.model_type}, timeout: {config.timeout}s)")
            return llm
            
        else:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ¨¡å‹é…ç½®: provider={config.provider}, "
                f"model_type={config.model_type}"
            )
    
    @staticmethod
    def select_embedding_model(model_name: str, device: Optional[str] = None) -> 'SentenceTransformer':
        """
        é€‰æ‹©å¹¶åˆå§‹åŒ– Embedding æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§° (å¦‚ "bge-large-zh-v1.5")
            device: è®¾å¤‡ (cpu/cuda/mps)ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ RUNNING_MODE
            
        Returns:
            SentenceTransformer å®ä¾‹
        """
        # å¦‚æœæ²¡æœ‰æŒ‡å®šè®¾å¤‡ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
        if device is None:
            device = RUNNING_MODE
        
        # è·å–æ¨¡å‹é…ç½®
        config = get_embedding_model(model_name)
        
        # åˆå§‹åŒ–æ¨¡å‹
        from sentence_transformers import SentenceTransformer
        
        model = SentenceTransformer(
            config.model_path,
            device=device
        )
        
        logger.info(f"âœ“ å·²åŠ è½½ Embedding æ¨¡å‹: {model_name}")
        logger.info(f"  ç»´åº¦: {config.dimension}, æœ€å¤§é•¿åº¦: {config.max_length}, è®¾å¤‡: {device}")
        print(f"âœ“ å·²åŠ è½½ Embedding æ¨¡å‹: {model_name} (è®¾å¤‡: {device})")
        
        return model
    
    @staticmethod
    def select_reranker_model(model_name: str, device: Optional[str] = None) -> 'FlagReranker':
        """
        é€‰æ‹©å¹¶åˆå§‹åŒ– Reranker æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§° (å¦‚ "bge-reranker-v2-m3")
            device: è®¾å¤‡ (cpu/cuda/mps)ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ RUNNING_MODE
            
        Returns:
            FlagReranker å®ä¾‹
        """
        # å¦‚æœæ²¡æœ‰æŒ‡å®šè®¾å¤‡ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
        if device is None:
            device = RUNNING_MODE
        
        # è·å–æ¨¡å‹é…ç½®
        config = get_reranker_model(model_name)
        
        # åˆå§‹åŒ–æ¨¡å‹
        from FlagEmbedding import FlagReranker
        
        model = FlagReranker(
            config.model_path,
            use_fp16=config.use_fp16,
            device=device
        )
        
        logger.info(f"âœ“ å·²åŠ è½½ Reranker æ¨¡å‹: {model_name}")
        logger.info(f"  æœ€å¤§é•¿åº¦: {config.max_length}, è®¾å¤‡: {device}")
        print(f"âœ“ å·²åŠ è½½ Reranker æ¨¡å‹: {model_name} (è®¾å¤‡: {device})")
        
        return model
    
    @staticmethod
    def get_model_config(model_name: str, model_type: str) -> Union[LLMModelConfig, EmbeddingModelConfig, RerankerModelConfig]:
        """
        è·å–æ¨¡å‹é…ç½®
        
        Args:
            model_name: æ¨¡å‹åç§°
            model_type: æ¨¡å‹ç±»å‹ ("llm", "embedding", "reranker")
            
        Returns:
            æ¨¡å‹é…ç½®å¯¹è±¡
        """
        if model_type == "llm":
            return get_llm_model(model_name)
        elif model_type == "embedding":
            return get_embedding_model(model_name)
        elif model_type == "reranker":
            return get_reranker_model(model_name)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    @staticmethod
    def list_all_models():
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        return {
            "llm": list_llm_models(),
            "embedding": list_embedding_models(),
            "reranker": list_reranker_models()
        }
    
    @staticmethod
    def list_llm_models_by_type(model_type: str):
        """
        æ ¹æ®ç±»å‹åˆ—å‡º LLM æ¨¡å‹
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ("local" æˆ– "cloud")
            
        Returns:
            List[LLMModelConfig]: åŒ¹é…ç±»å‹çš„æ¨¡å‹é…ç½®åˆ—è¡¨
        """
        all_models = list_llm_models()
        return [m for m in all_models if m.model_type == model_type]
    
    @staticmethod
    def get_model_info(model_name: str, model_category: str) -> dict:
        """
        è·å–æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            model_name: æ¨¡å‹åç§°
            model_category: æ¨¡å‹ç±»åˆ« ("llm", "embedding", "reranker")
            
        Returns:
            dict: æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        config = ModelManager.get_model_config(model_name, model_category)
        return config.to_dict()



# ==================== å¯¼å‡ºç»Ÿä¸€ç®¡ç†å™¨å®ä¾‹ ====================

model_manager = ModelManager()

